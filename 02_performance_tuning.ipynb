{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORF5ir1OSh1iIx5xAIxnB3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydmarinb/spark/blob/main/02_performance_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance tuning\n",
        "\n",
        "El **performance tuning** en PySpark se refiere a optimizar y ajustar el rendimiento de las aplicaciones de PySpark para lograr un procesamiento más rápido y eficiente de los datos. Dado que PySpark opera en un entorno distribuido utilizando el framework de procesamiento distribuido Apache Spark, hay varios aspectos que se deben considerar para maximizar su rendimiento. Aquí tienes algunas estrategias y técnicas comunes para la optimización de rendimiento en PySpark:\n",
        "\n",
        "\n",
        "**Particiones adecuadas:**\n",
        "Las particiones son fragmentos de datos que se procesan en paralelo. Si hay un número insuficiente de particiones, no se aprovechará completamente el paralelismo. Si hay demasiadas, podría aumentar la sobrecarga de administración de particiones. Utiliza repartition para redistribuir los datos en un número deseado de particiones.\n",
        "\n",
        "**Tamaño de la partición:**\n",
        "El tamaño de las particiones es importante. Las particiones pequeñas pueden llevar a una sobrecarga debido a la administración excesiva de particiones, mientras que las particiones grandes pueden subutilizar recursos. Ajusta el tamaño según la memoria de los nodos y la naturaleza de los datos.\n",
        "\n",
        "**Uso de caché:**\n",
        "La caché es útil cuando planeas usar un DataFrame múltiples veces, evitando recomputaciones. Usa cache o persist con nivel de almacenamiento adecuado (MEMORY_ONLY, MEMORY_AND_DISK, etc.). Sin embargo, ten en cuenta que esto consume memoria.\n",
        "\n",
        "**Broadcasting:**\n",
        "Las tablas pequeñas pueden enviarse a cada nodo a través de broadcasting en lugar de redistribuir a través de la red. Utiliza broadcast para indicar que un DataFrame debe ser tratado como una tabla broadcast.\n",
        "\n",
        "**Operaciones de transformación vs. acción:**\n",
        "Spark realiza operaciones de transformación de manera perezosa, acumulando un plan de ejecución. Minimiza las acciones innecesarias para evitar la recomputación. Planea cuidadosamente las acciones para optimizar el flujo de trabajo.\n",
        "\n",
        "**Uso eficiente de memoria:**\n",
        "Configura correctamente la asignación de memoria para el driver y los ejecutores. Controla la proporción de memoria reservada para almacenamiento y ejecución. Ajusta spark.driver.memory, spark.executor.memory, spark.memory.fraction, etc.\n",
        "\n",
        "**Configuración de recursos:**\n",
        "Ajusta el número de nodos ejecutores, núcleos por ejecutor y memoria asignada según el tamaño del clúster y la naturaleza de la tarea. Asegúrate de no asignar demasiados recursos, lo que podría llevar a la competencia por recursos.\n",
        "\n",
        "**Shuffling eficiente:**\n",
        "Shuffling, la redistribución de datos entre particiones, puede ser costoso. Utiliza transformaciones como reduceByKey, aggregateByKey y combineByKey para reducir la necesidad de shuffling. Utiliza persist o cache después de una operación de shuffling para evitar recalculos.\n",
        "\n",
        "**Formatos de almacenamiento:**\n",
        "Utiliza formatos de almacenamiento eficientes como Parquet. Parquet es columnar y admite compresión, lo que mejora la velocidad de lectura y reduce el uso de almacenamiento.\n",
        "\n",
        "**Optimización de consultas:**\n",
        "Si utilizas Spark SQL, optimiza tus consultas. Utiliza EXPLAIN para entender el plan de ejecución. Utiliza índices o reorganiza tus datos si es posible para mejorar el rendimiento de las consultas.\n",
        "\n",
        "**Monitoreo y ajuste:**\n",
        "Utiliza las herramientas de monitoreo de Spark, como la interfaz web Spark UI, para rastrear el progreso de la aplicación, el uso de recursos, los tiempos de ejecución y más. Ajusta tu configuración y estrategias según lo que aprendas del monitoreo.\n",
        "\n"
      ],
      "metadata": {
        "id": "tOprmf8W9_9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pedendencies"
      ],
      "metadata": {
        "id": "j3oG7fVIjlAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "tmiQzaOtjwCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define enviroment variables"
      ],
      "metadata": {
        "id": "ZmiX8mZMjxj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "2j6sGqxtjzTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqMHRjdj2ID",
        "outputId": "489e3fe2-2a64-43ec-8ad9-23d5d05f9ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=617f6cf437541790b53991bbe0d0c9101a6261da1e0945f6c1c8860390e03e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1YiwR3a-j3ab",
        "outputId": "d540cdfb-c06a-4ea0-fed3-8aaea5cc4703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78d13a8c1300>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8f66e1473ed3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDJR1y-tj5Fg",
        "outputId": "c95718e3-ebd2-402d-b352-fea1e9c23613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir = '/content/drive/MyDrive/retail_db'"
      ],
      "metadata": {
        "id": "Uhpu72_Hj952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catalys optimizer\n",
        "\n",
        "<img src=\"https://www.databricks.com/wp-content/uploads/2018/05/Catalyst-Optimizer-diagram.png\">\n",
        "\n",
        "Para poder observar el diagrama anterior:\n",
        "\n",
        "> Ir a la interface de apache -> SQL/DataFrame -> Seleccionar la operación de interes -> Details.\n",
        "\n",
        "Wl objetivo en este caso es conocer el detalle de la ejecución de un proceso que esta llevando a cabo spark y asi poder optimizarlos.\n",
        "\n"
      ],
      "metadata": {
        "id": "BypUb_xUyUFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leyendo csv, y definiendo el tipo de datos y el nombre de columnas con el comando schema\n",
        "df = spark.read.csv(f'{base_dir}/orders', schema='order_id INT, order_date DATE, order_customer_id INT, order_status STRING')\n"
      ],
      "metadata": {
        "id": "xBwl4e_X_tEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import count, col\n",
        "\n",
        "df.\\\n",
        "  groupBy('order_status').\\\n",
        "    agg(count('order_id').alias('order_count')).\\\n",
        "      orderBy(col('order_count').desc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "p49K3qmA_5mn",
        "outputId": "ebcc9a4e-2766-45bd-8866-61a68021e5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------------+-----------+\n",
              "|   order_status|order_count|\n",
              "+---------------+-----------+\n",
              "|       COMPLETE|      22899|\n",
              "|PENDING_PAYMENT|      15030|\n",
              "|     PROCESSING|       8275|\n",
              "|        PENDING|       7610|\n",
              "|         CLOSED|       7556|\n",
              "|        ON_HOLD|       3798|\n",
              "|SUSPECTED_FRAUD|       1558|\n",
              "|       CANCELED|       1428|\n",
              "| PAYMENT_REVIEW|        729|\n",
              "+---------------+-----------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>order_status</th><th>order_count</th></tr>\n",
              "<tr><td>COMPLETE</td><td>22899</td></tr>\n",
              "<tr><td>PENDING_PAYMENT</td><td>15030</td></tr>\n",
              "<tr><td>PROCESSING</td><td>8275</td></tr>\n",
              "<tr><td>PENDING</td><td>7610</td></tr>\n",
              "<tr><td>CLOSED</td><td>7556</td></tr>\n",
              "<tr><td>ON_HOLD</td><td>3798</td></tr>\n",
              "<tr><td>SUSPECTED_FRAUD</td><td>1558</td></tr>\n",
              "<tr><td>CANCELED</td><td>1428</td></tr>\n",
              "<tr><td>PAYMENT_REVIEW</td><td>729</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el physical plan\n",
        "df.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81NrHXBE-E7U",
        "outputId": "e282a29b-f461-49d7-9bd6-5a8bb6b4e3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [order_id#0,order_date#1,order_customer_id#2,order_status#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/drive/MyDrive/retail_db/orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,order_date:date,order_customer_id:int,order_status:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning Columnar format\n",
        "\n",
        "Una de las estrategias para evitar el sobre proceso cuando se trabajan con archivos planos, es crear particiones de los datos como se muestra a continuación."
      ],
      "metadata": {
        "id": "-bfSgoovAEYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.\\\n",
        "  write . \\\n",
        "  partitionBy('Year', 'Month'). \\\n",
        "  mode('overwrite'). \\\n",
        "  parquet('dbfs:/FileStore/db')"
      ],
      "metadata": {
        "id": "ugAybKntFlJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}